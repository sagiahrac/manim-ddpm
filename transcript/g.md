[Title appears: â€œDDPM Innovationâ€]
â€œNow that weâ€™ve seen how diffusion models work in general, letâ€™s look at what the original DDPM paper brought to the table.â€

[Noisy corgi with label 
ğ‘¥
ğ‘¡
x 
t
â€‹
  appears]
â€œIn standard diffusion models, the reverse step tries to directly predict the denoised image at each stage.
The DDPM paper took a different approach.â€

[Arrow down to corgi with fewer dots, label 
ğ‘¥
ğ‘¡
âˆ’
1
x 
tâˆ’1
â€‹
 ]
â€œInstead of guessing the clean image, the model predicts the noise that was added â€” 
ğœ–
Ïµ.â€

[Extra noise separates to the right, minus sign appears, noise labeled 
ğœ–
Ïµ]
â€œThis is powerful because the forward process already tells us exactly how noise was injected at every step.
If we can recover 
ğœ–
Ïµ accurately, the rest follows from a simple re-parameterization.â€

[Hold on 
ğ‘¥
ğ‘¡
âˆ’
ğœ–
â†’
ğ‘¥
ğ‘¡
âˆ’
1
x 
t
â€‹
 âˆ’Ïµâ†’x 
tâˆ’1
â€‹
 ]
â€œThatâ€™s the core innovation of DDPM: reframing denoising as noise prediction.
And if you think about it, itâ€™s a bit like skip connections in residual networks â€” instead of predicting the whole output, you predict just the residual.
It makes training more stable, the loss more aligned with likelihood,
and connects directly to a widely used approach in machine learning: denoising score matching.
